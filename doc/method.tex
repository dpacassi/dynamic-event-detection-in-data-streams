\section{Method}

% TODO define proper outline

% TODO introductionary paragraph

\subsection{Test Data}

The first important step is to create a test data set to run the evaluations with and verify them.

% TODO explain the source and structrue

\subsection{Clustering methods}

Clustering finds similarities in different news articles based on their content and groups them together in so called clusters, while unrelated news are regarded as noise. The challenge now araises to find an appropriate clustering method, which is able to work with data of varying densities and of high dimensionality.

An initial evaluation (Table \ref{tab:first_evaluation}) shows a selection of potential clustering methods. The selection was done based on the criteria, that the number of clusters is unkown. Many clustering methods as for example K-Means require to know the number of clusters, which we cannot provide once we work with dynamic data. Based on this first evaluation and its property to be able to handle noise, we decided to focus on HDBSCAN as our main clustering method. Further evaluations will be done in comparison with K-Means, since it is one of the most commonly used clustering algorithm [insert citation]. K-Means will be run with the true number of clusters and an approximation based on $\sqrt{n}$.

% add number of clusters into table
\begin{table}[h]
    \centering
    \label{tab:first_evaluation}
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Algorithm} & \textbf{NMI} & \textbf{Completeness}  & \textbf{Average Processing Time (in seconds)} \\ \hline
    affinity\_propagation & \textbf{0.74} & 0.51 & 14.42 \\ \hline
    hdbscan & 0.73 & \textbf{0.68} & \textbf{0.3} \\ \hline
    birch & 0.64 & 0.14 & 1.27 \\ \hline
    spectral\_clustering & 0.55 & 0.44  & 461.21 \\ \hline
    hdbscan\_lda & 0.51 & 0.43 & 154.53 \\ \hline
    meanshift & 0.33 & 0.06  & 440.74 \\ \hline
    \end{tabular}
    \caption{Write a caption}
\end{table}

\subsubsection{HDBSCAN}

HDBSCAN is a hierarchical density-based clustering algorithm \cite{McInnes2017}. It extends the well known [insert citation] DBSCAN algorithm and reduces its sensitivity for clusters of varying densities. Another important quality of HDBSCAN is, that it does not need to know the number of clusters up front.

% TODO explain some more

\subsubsection{K-means}

KMeans is a centroid-based clustering algorithm. 

% TODO explain some more

\subsubsection{Preprocessing}

% raw text
% with entity extraction
% word embeddings?
% Tfidf

\subsubsection{Score calculation}

The scoring function is essential for measuring the result of a clustering method. The score should tell us how close the resulting clusters are to the ground truth and give us an intuitive understanding about the quality of the clusters. One of the challenges when comparing one clustering with the other, is that the differnet clusters do not share the same label across different clusterings. Which is why scoring functions used for clusterings typically ignore the labels and focus on the shape of a cluster.

Initially we used Normalized Mutual Information (NMI) as our primary scoring function. The NMI calculates the mutual information between two clusterings and normalizes the result by dividing through a generalized mean of the entropy of each clustering. % TODO better description.
The score proved to work well for our initial evaluations and manual samplings verified the score. Upon running evaluations with higher sample sizes the NMI score started to lose its reliability. An example is given in Table \ref{tab:nmi_kmeans_example}, where K-means achieved a rather high score, regardless to the significant difference between the true amount of clusters and the approximation using $\sqrt{n}$.

% TODO explain the reason for the big difference

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Algorithm} & \textbf{Sample Size} & \textbf{NMI}  & $\mathbf{ \mid n_{true} - n_{predicted} \mid }$ \\ \hline
    k-means & 19255 & 0.754 & 457 \\ \hline
    hdbscan & 19255 & 0.742 & 2 \\ \hline
    \end{tabular}
    \caption{K-Means has a higher NMI score than HDBSCAN, while having a much larger difference in number of clusters.}
    \label{tab:nmi_kmeans_example}
\end{table}

To get a more accurate score based on the documents inside a cluster, we developed our own scoring function. The scoring function first calculates the accuracy for each combination of two clusters, where each clusters belongs to a different clustering. We define the accuracy as

\begin{equation}
    \label{equ:accuracy}
        \frac{n_{true}}{n_{true} + fp + fn}
    \end{equation}

where $n_{true}$ is the number of elements in the true cluster, $fp$ is the number of false positives in the predicted cluster and $fn$ describes the number of false negatives in the predicted cluster. As a next step a selection process is used to find relevant accuracy values from each row of the accuracy matrix. The sum of the selected values is then divided by the number of rows, which results in the average accuracy of the current clustering method to evaluate. 

Finding relevant values in the accuracy matrix non-trivial, since clusters do not share labels across different clusterings. To solve this, we make two assumptions:
\begin{enumerate} 
\item The higher the accuracy between two clusters, the more likely it is, that both clusters are describing the same group of documents. 
\item Each cluster can be associated with a cluster from another clustering only once.
\end{enumerate}

Based on those assumptions we select the highest accuracy value per row, whose column is not already associated with another row. Listing \ref{lst:select_max_values} shows the selection process in more detail. 

\begin{lstlisting}[language=Python, caption=Select relevant accuracy values from a accuracy matrix., label={lst:select_max_values}]
def select_max_values(self, accuracy_matrix):
    unique_indicies = dict()
    row_index = 0
    nrows = len(accuracy_matrix)
    
    while row_index < nrows:
        ignore_indicies = set()
        max_value_found = False

        while not max_value_found:
            max_value = 0
            column = 0
            for col_index, value in enumerate(accuracy_matrix[row_index]):
                if value >= max_value and col_index not in ignore_indicies:
                    max_value = value
                    column = col_index

            if (
                max_value > 0
                and column in unique_indicies
                and unique_indicies[column]["row_index"] != row_index
                and unique_indicies[column]["max_value"] > 0
            ):
                if unique_indicies[column]["max_value"] < max_value:
                    # The column is already used, but we found a better 
                    # candidate. We use the new candidate and set the 
                    # cursor to the old one to find a new max value.
                    old_row_index = unique_indicies[column]["row_index"]
                    unique_indicies[column]["row_index"] = row_index
                    row_index = old_row_index
                    unique_indicies[column]["max_value"] = max_value
                    max_value_found = True
                else:
                    # The column is already used by a better candidate.
                    ignore_indicies.add(column)
            else:
                # If max_value is greater than 0, we store the value as a 
                # new candiate. Otherwise either the row does not match 
                # any other column or the max_value was low and got 
                # overriden by previous tries and no other match is available. 
                if max_value > 0:
                    # The column is free to use
                    unique_indicies[column] = {
                        "row_index": row_index,
                        "max_value": max_value,
                    }
                max_value_found = True
                row_index += 1
    
    return unique_indicies
\end{lstlisting}

After repeating the evaluation shown in Table \ref{tab:nmi_kmeans_example} a second time using the average accuracy per clustering, the score (Table \ref{tab:avg_predict_kmeans_example}) for K-means is much lower than HDBSCAN. This reflects what we would expect based on the big difference in the amount of predicted clusters.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Algorithm} & \textbf{Sample Size} & \textbf{AVG Accuracy}  & $\mathbf{ \mid n_{true} - n_{predicted} \mid }$ \\ \hline
    k-means & 19255 & 0.137 & 457 \\ \hline
    hdbscan & 19255 & 0.605 & 2 \\ \hline
    \end{tabular}
    \caption{K-Means has a higher NMI score than HDBSCAN, while having a much larger difference in number of clusters.}
    \label{tab:avg_predict_kmeans_example}
\end{table}

    
\subsubsection{Evaluation}


\subsection{Online Clustering}

* time based sliding window
* Near duplicate detection for clusters with MinHash and LSH O(log n)
* Jaccard Coefficient: number of common elements / (total number of elements - number of common elements)
* source https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134

