\subsection{Software}

\subsubsection{Evaluation Framework}
The evaluation process is done with our own evaluation framework.
The framework allows for automated and repeatable evaluation runs.
Results are stored in a database for later analysis.
The main features include:

\begin{itemize}
    \item Defining the number of stories to run the evaluation with and load all news articles from those stories.
    \item Repeating evaluation runs with different sets of data.
    \item Providing different vectorizers for converting the textual data into a vector space model.
    \item Defining a range for each parameter of a clustering method and running it with each possible combination of those parameters.
    \item Storing the result the result in a database and creating relations between news articles, clusters and evaluation runs. This allows for manual inspection and analysis of individual articles inside a predicted cluster.
\end{itemize}

The implementation is done with Python. Clustering methods and vectorizers are provided by the Scikit-learn library. We decided to use Scikit-learn because of its rich documentation, the wide range of tools and algorithms it provides for clustering and our previous experience with it. Additionally the framework runs in a fully dockerized environment, which includes the database. This makes it very easy to run locally or on a server.

% TODO write it better

\subsubsection{Online Clustering}

* Near duplicate detection for clusters with MinHash and LSH O(log n)
* source https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134

* simulate an dynamic stream from our static dataset
