\subsection{Data Set}
\label{subsec:4a_data_set}

Before any clustering method can be implemented or evaluated,
it is important to rely on the right data set for training and evaluation.

\subsubsection{Data Set Candidates}
\label{subsubsec:4a_data_set_candidates}

As our goal is to detect events in data streams, we've evaluated different data sets and
their possibilities to extract events from their data themselves.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|r|l|}
    \hline
    \textbf{Data set} & \textbf{Number of rows} & \textbf{Description} \\ \hline
    GDELT 2.0 & 575'000'000+ & Print and web news from around the world. \\ \hline
    ChallengeNetwork & 4'449'294 & Network packages including anomalies. \\ \hline
    One Million Posts Corpus & 1'011'773 & User comments to news articles. \\ \hline
    Online Retail Data Set & 541'909 & Customer retail purchases of one year. \\ \hline
    News Aggregator Dataset & 422'937 & Clustered news articles. \\ \hline
    Dodgers Loop Sensor Data Set & 50'400 & Number of cars driven through a ramp. \\ \hline
    10k German News Articles & 10'273 & German news articles. \\ \hline
    \end{tabular}
    \caption{Evaluated data set candidates ordered by data set size.}
    \label{tab:data_set_candidates}
\end{table}

We could extract events from all data sets mentioned in Table \ref{tab:data_set_candidates}.\\
The extracted events could be as follows:

\begin{itemize}
    \item Network packages
        \begin{itemize}
            \item Cyber attacks depending on suspicious packets.
        \end{itemize}
    \item User comments
        \begin{itemize}
            \item Change of public opinion during time.
        \end{itemize}
    \item Retail purchases
        \begin{itemize}
            \item Change of purchasing behavior based on product choices.
        \end{itemize}
    \item Traffic
        \begin{itemize}
            \item Traffic changes due to baseball games.
        \end{itemize}
    \item News articles
        \begin{itemize}
            \item Development of a certain news story.
        \end{itemize}
\end{itemize}

However, from above data sets only two contained prelabeled events:

\begin{enumerate}
    \item Dodgers Loop Sensor Data Set
        \begin{itemize}
            \item 81 labeled events. % Todo: x events in y clusters
        \end{itemize}
    \item News Aggregator Dataset
        \begin{itemize}
            \item 422'937 labeled events. % Todo: x events in y clusters
        \end{itemize}
\end{enumerate}

As we didn't want to lose too much time in manually clustering data, we've decided to go with one of these two.
Regarding our two options, our choice was simple:\\
We went for the \textbf{The News Aggregator Dataset} since it not only provided more data,
but our work built on the news articles use case could later be continued with real live data.
The \textbf{GDELT 2.0} data set for example, provides around 1'000 to 2'000 new news articles every 15 minutes.

\subsubsection{Data Retrieval}
\label{subsubsec:4a_data_retrieval}

Unfortunately the data set did not contain the news articles themselves but rather only the URL's to the news articles.
This was done so due to copyright restrictions on the content.
Fortunately there are web scraping tools designed to retrieve the content from news articles specifically.
We decided to use Newspaper3k\cite{newspaper3k},
a Python3 library that allows us to retrieve the text from news articles easily.

The library only requires an URL to download and extract the news article from a website, see following example:

% Todo: Hyphen gets converted inside "lstlisting". Copy & paste the URL from the PDF into a browser -> it won't work.
\begin{lstlisting}[language=Python, caption=Retrieve the news article from an URL., label={lst:newspaper3k_code}]
    from newspaper import Article

    url = 'http://fox13now.com/2013/12/30/new-year-new-laws-obamacare-pot-guns-and-drones/'
    article = Article(url)
    article.download()
    article.text # Contains the article's text.
\end{lstlisting}

All we had to do now is to run this code for all news articles.
To speed this process up, we've loaded the data set into a database and run 8 concurrent processes which
retrieved the news articles content from the web in different batches.

\subsubsection{Data Cleansing}
\label{subsubsec:4a_data_cleansing}

The data set contains news articles collected from March 10th to August 10th of 2014.
Five years later, many resources are not online anymore or are not accessible from Europe due to GDPR.
We've used following SQL query to filter out news articles that were most likely corrupt:

\begin{lstlisting}[language=SQL, caption=Retrieve valid news articles., label={lst:valid_news_articles_sql}]
    SELECT *
    FROM news_article
    WHERE
        newspaper_text IS NOT NULL
        AND TRIM(COALESCE(newspaper_text, '')) != ''
        AND hostname NOT IN ('newsledge.com', 'www.newsledge.com')
        AND newspaper_text NOT LIKE '%GDPR%'
        AND newspaper_text NOT LIKE '%javascript%'
        AND newspaper_text NOT LIKE '%404%'
        AND newspaper_text NOT LIKE '%cookie%'
        AND newspaper_keywords NOT LIKE '%GDPR%'
        AND newspaper_keywords NOT LIKE '%javascript%'
        AND newspaper_keywords NOT LIKE '%404%'
        AND newspaper_keywords NOT LIKE '%cookie%'
        AND title_keywords_intersection = 1
\end{lstlisting}

% Todo: Continue cleansing text.

% Todo: The first important step is to create a test data set to run the evaluations with and verify them.
% Todo: explain the source and structrue

% raw text
% with entity extraction
% word embeddings?
% Tfidf
