\section{Theoretical basics}
Todo.

\subsection{Text preprocessing}
Todo.

\subsubsection{Keyterm extraction}
Todo.

\subsubsection{Entity extraction}
Todo.
% TODO: "Named entity recognition"?

\subsubsection{Text lemmatization}
Todo.

\subsubsection{Text stemming}
Todo.

\subsection{Data Representation}
Todo.

\subsubsection{Word Frequency}
Todo.

\subsubsection{tf-idf}
Todo.

\subsection{Clustering}
Clustering finds similarities in different news articles based on their content and groups them together,
while unrelated news are regarded as noise.
The challenge now arises to find an appropriate clustering method,
which is able to work with data of varying densities and of high dimensionality.

% TODO why hdbscan

\subsubsection{\textit{k}-means clustering}
\textit{k}-means clustering is an iterative clustering method which assigns all data points in a given data set
into k clusters, where k is a predefined number of clusters in the dataset.

\paragraph{How does k-means clustering work}
At the very beginning, k-means creates k centroids at random locations.
It then repeats following instructions until reaching convergence:

\begin{itemize}
    \item For each data point: Find the nearest centroid
    \item Assign the data point to the nearest centroid (cluster)
    \item For each cluster: Compute a new cluster centroid with all assigned data points
\end{itemize}

\paragraph{Advantages}
\begin{itemize}
    \item Very simple and easy to understand algorithm
\end{itemize}

\paragraph{Disadvantages}
\begin{itemize}
    \item Initial (random) centroids have a strong impact on the results
    \item The number of clusters (k) has to be known beforehand
    \item Unable to handle noise (all data points will be assigned to a cluster)
\end{itemize}

\subsubsection{DBSCAN}
DBSCAN stands for \textit{Density-Based Spatial Clustering of Applications with Noise}
and is a density based clustering algorithm.

A big advantage of DBSCAN is that it's able to sort data into clusters
of different shapes.

\paragraph{How does DBSCAN work}
DBSCAN requires two parameters in order to work:

\begin{enumerate}
    \item epsilon - The maximum distance between two data points for them to be considered as in the same cluster.
    \item minPoints - The number of data points a neighbourhood has to contain in order to be considered as a cluster.
\end{enumerate}

Having these two parameters defined, DBSCAN will iterate through the data points
and try to assign them to clusters if the provided parameters match.
If a data point can't be assigned to a cluster, it will be marked as noise point.

Data points that belong to a cluster but don't dense themselves are known as \textbf{border points}.
Some border points could theoretically belong to two or more clusters
if the distance from the point to the clusters don't differ.

\paragraph{Advantages}
\begin{itemize}
    \item Does not need to know the number of clusters beforehand.
    \item Is able to find shaped clusters.
    \item Is able to handle noise points.
\end{itemize}

\paragraph{Disadvantages}
\begin{itemize}
    \item DBSCAN is not entirely deterministic.
    \item Defining the right epsilon value can be difficult.
    \item Unable to cluster data sets with large differences in densities.
\end{itemize}

\subsubsection{HDBSCAN}

% Todo: check if we can re-use something of the commented draft.
\iffalse
HDBSCAN is a hierarchical density-based clustering algorithm \cite{McInnes2017}.
It extends the well known [insert citation] DBSCAN algorithm and reduces its sensitivity for clusters of varying densities.
Another important quality of HDBSCAN is, that it does not need to know the number of clusters up front.
\fi

HDBSCAN is a extension of DBSCAN that converts DBSCAN into a hierarchical clustering algorithm.
Therefore it only requires one parameter to be set:
\begin{enumerate}
    \item minPoints - The number of data points a neighbourhood has to contain in order to be considered as a cluster.
\end{enumerate}

\paragraph{How does HDBSCAN work}
\begin{enumerate}
    \item Transform the space according to the density/sparsity.
    \item Build the minimum spanning tree of the distance weighted graph.
    \item Construct a cluster hierarchy of connected components.
    \item Condense the cluster hierarchy based on minimum cluster size.
    \item Extract the stable clusters from the condensed tree.
\end{enumerate}

\subparagraph{1. Transforming the space}
One reason why HDBSCAN works so well, is that it is aware of noise.
This is very important as real life data is often messy and sometimes even corrupt.

Because of this, the very first thing HDBSCAN does, is to remove such data noise.
In order to identify noise, the \textbf{mutual reachability distance} score between
points is calculated.
%The mutual reachability distance score is defined as

% TODO: Formula

%whereas corek(x) stands for the core distance for a point x and a defined parameter k.

% \subsubsubsubsection{2. Building the minimum spanning tree}
Todo.
