\section{Theoretical basics}
Todo.

\subsection{Text preprocessing}
Todo.

\subsubsection{Keyterm extraction}
Todo.

\subsubsection{Entity extraction}
Todo.
% TODO: "Named entity recognition"?

\subsubsection{Text Stemming}
% TODO: cite https://tartarus.org/martin/PorterStemmer/
Text Stemming is a form of Text Normalization which aims to simplify words
by reducing the inflectional forms of each word into their word stems.
The original algorithm paper was written in 1979 by Martin Porter who aimed
to improve performance of information retrieval systems.
He stated that words such as \textit{connect}, \textit{connected}, \textit{connecting},
\textit{connection} and \textit{connections} will usually have a similar meaning and can therefor
be simplified for text vectorisation.
In his paper he was able to process his suffix stripping algorithm in 6'370 out of 10'000 words and thereby
reducing the vocabulary size by one third.

% TODO: cite https://pdfs.semanticscholar.org/1c0c/0fa35d4ff8a2f925eb955e48d655494bd167.pdf
% http://snowball.tartarus.org/texts/quickintro.html
% http://snowball.tartarus.org/texts/introduction.html
\paragraph{Snowball stemmer}
Todo

\subsubsection{Text Lemmatisation}
Text Lemmatisation is a form of Text Normalization which aims to simplify words
by reducing the inflectional forms of each word into a common base or root.

\subsection{Data Representation}
Todo.

\subsubsection{Word Frequency}
Todo.

\subsubsection{tf-idf}
Todo.

\subsection{Clustering}
Clustering finds similarities in different news articles based on their content and groups them together,
while unrelated news are regarded as noise.
The challenge now arises to find an appropriate clustering method,
which is able to work with data of varying densities and of high dimensionality.

% TODO why hdbscan

\subsubsection{\textit{k}-means clustering}
\textit{k}-means clustering is an iterative clustering method which assigns all data points in a given data set
into k clusters, where k is a predefined number of clusters in the data set.

\paragraph{How does k-means clustering work}
At the very beginning, k-means creates k centroids at random locations.
It then repeats following instructions until reaching convergence:

\begin{itemize}
    \item For each data point: Find the nearest centroid
    \item Assign the data point to the nearest centroid (cluster)
    \item For each cluster: Compute a new cluster centroid with all assigned data points
\end{itemize}

\paragraph{Advantages}
\begin{itemize}
    \item Very simple and easy to understand algorithm
\end{itemize}

\paragraph{Disadvantages}
\begin{itemize}
    \item Initial (random) centroids have a strong impact on the results
    \item The number of clusters (k) has to be known beforehand
    \item Unable to handle noise (all data points will be assigned to a cluster)
\end{itemize}

\subsubsection{DBSCAN}
DBSCAN stands for \textit{Density-Based Spatial Clustering of Applications with Noise}
and is a density based clustering algorithm.

A big advantage of DBSCAN is that it's able to sort data into clusters
of different shapes.

\paragraph{How does DBSCAN work}
DBSCAN requires two parameters in order to work:

\begin{enumerate}
    \item epsilon - The maximum distance between two data points for them to be considered as in the same cluster.
    \item minPoints - The number of data points a neighbourhood has to contain in order to be considered as a cluster.
\end{enumerate}

Having these two parameters defined, DBSCAN will iterate through the data points
and try to assign them to clusters if the provided parameters match.
If a data point can't be assigned to a cluster, it will be marked as noise point.

Data points that belong to a cluster but don't dense themselves are known as \textbf{border points}.
Some border points could theoretically belong to two or more clusters
if the distance from the point to the clusters don't differ.

\paragraph{Advantages}
\begin{itemize}
    \item Does not need to know the number of clusters beforehand.
    \item Is able to find shaped clusters.
    \item Is able to handle noise points.
\end{itemize}

\paragraph{Disadvantages}
\begin{itemize}
    \item DBSCAN is not entirely deterministic.
    \item Defining the right epsilon value can be difficult.
    \item Unable to cluster data sets with large differences in densities.
\end{itemize}

\subsubsection{HDBSCAN}

% Todo: check if we can re-use something of the commented draft.
\iffalse
HDBSCAN is a hierarchical density-based clustering algorithm \cite{McInnes2017}.
It extends the well known [insert citation] DBSCAN algorithm and reduces its sensitivity for clusters of varying densities.
Another important quality of HDBSCAN is, that it does not need to know the number of clusters up front.
\fi

HDBSCAN is a extension of DBSCAN that converts DBSCAN into a hierarchical clustering algorithm.
Therefore it only requires one parameter to be set:
\begin{enumerate}
    \item minPoints - The number of data points a neighbourhood has to contain in order to be considered as a cluster.
\end{enumerate}

\paragraph{How does HDBSCAN work}
\begin{enumerate}
    \item Transform the space according to the density/sparsity.
    \item Build the minimum spanning tree of the distance weighted graph.
    \item Construct a cluster hierarchy of connected components.
    \item Condense the cluster hierarchy based on minimum cluster size.
    \item Extract the stable clusters from the condensed tree.
\end{enumerate}

\subparagraph{1. Transforming the space}
One reason why HDBSCAN works so well, is that it is aware of noise.
This is very important as real life data is often messy and sometimes even corrupt.

Because of this, the very first thing HDBSCAN does, is to remove such data noise.
In order to identify noise, the \textbf{mutual reachability distance} score between
points is calculated.
%The mutual reachability distance score is defined as

% TODO: Formula

%whereas corek(x) stands for the core distance for a point x and a defined parameter k.

% \subsubsubsubsection{2. Building the minimum spanning tree}
Todo.
